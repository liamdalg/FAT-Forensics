# Product Evaluation

Our tool is bespoke and oriented at experts who are already aware predictive algorithms and what it means to measure their FAT. As a result, it was difficult to evaluate the success of our product without explaining the intent of the product.

We opted to only consult the client to evaluate whether we have met the requirements of the product due to its difficult nature. During development, we met our client frequently and we were in contact via email so he has an updated information about our processes. We presented our product to the client after most of the implementation was finished, he was very impressed and gave us this statement:

> The tool was demonstrated to me on the 29th of April 2019. The team did a great job and I was very impressed by the demo shown to me on that day. The tool has all the components that were discussed with the team throughout the project development phase and the team has delivered what was promised to us. We have set a date for the handover of the project for us to be able to integrate it with our toolbox once it is released to the public. The team has delivered a strong foundation on which we hope to develop a fully-featured web interface for our tool. We believe that it is a step in the right direction to make FAT research more reproducible and accessible to an audience who is not familiar with computer programming paradigms, e.g. lawyers and policymakers. Thank you for all your hard work!

Peer reviews are proved useless for evaluation of the tool functionality due to a lack of understanding of the product. However, they are useful for evaluating the UI and usability of the product. If we only consulted our client, then the design of our tool could be making assumptions about the skill-set and knowledge of the user which could make it difficult to use.

For our evaluation strategies, we asked our peers to perform basic tasks (e.g. add a node, execute graph) and listen to any feedback they have during the process. Additionally, we demoed our project and walked them through how it works. Most were impressed with the complexity of the system, and enjoyed the ease of navigation. On the other hand, they gave us many ideas on how to make the tool more intuitive, enabling us to create an internal list of features to be added or changes to be made.

Example feedback:

* The export functionality is not clear to the user.
* It is not clear what are valid values indices and axes.
* Double clicking on the graph should not zoom.
* Itâ€™s not clear that execute is required before inspect. There should be an option to execute within the inspector view.
