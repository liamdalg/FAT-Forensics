# Product Evaluation

Our tool is oriented at professionals who are aware of the implications of predictive algorithms and their FAT. As a result, it was difficult to evaluate the success of our product without arduous explaining of the intent of the product.

Because of this, we opted to only consult the client to evaluate whether we have met the requirements of the product functionality itself. During development, we contacted our client via email and met frequently to discuss the progress of the project. We presented our product to the client after most of the implementation was finished, he was very impressed and gave us this statement:

> The tool was demonstrated to me on the 29th of April 2019. The team did a great job and I was very impressed by the demo shown to me on that day. The tool has all the components that were discussed with the team throughout the project development phase and the team has delivered what was promised to us. We have set a date for the handover of the project for us to be able to integrate it with our toolbox once it is released to the public. The team has delivered a strong foundation on which we hope to develop a fully-featured web interface for our tool. We believe that it is a step in the right direction to make FAT research more reproducible and accessible to an audience who is not familiar with computer programming paradigms, e.g. lawyers and policymakers. Thank you for all your hard work!

We found that peer review techniques proved difficult for evaluation of the core functionality, since it required prior knowledge of what it means to measure the FAT of a predictive algorithm. However, they were useful for evaluating the UI and usability of the product. If we had only consulted our client, then the design of our tool could be making assumptions about how familiar the user is with the product.

For our evaluation strategies, we asked our peers to perform basic tasks (e.g. add a node, execute graph) and listen to any feedback they have during the process. Additionally, we introduced our product and walked them through how it works. Most were impressed with the complexity of the system and enjoyed the ease of navigation. On the other hand, they gave us many ideas on how to make the tool more intuitive, enabling us to create an internal list of features to be added or changes to be made.

Example feedback:

* The export functionality is not clear to the user.
* It is not clear what are valid values indices and axes.
* Double-clicking on the graph should not zoom.
* Itâ€™s not clear that execute is required before inspect. There should be an option to execute within the inspector view.
