# FAT-Forensics

Artificial Intelligence (AI) and in particular its part concerned with eliciting knowledge from data called Machine Learning (ML) has become ubiquitous in the past decade. Up until recently the main concern of ML system developers was their predictive power, i.e. how well they can identify a given pattern based on its features. However, when such systems started to take (often legally binding or life changing) decisions about humans, researchers realised that there is more to be tested about these algorithms then simply their predictive power. Based on stories such as COMPAS -- a proprietary algorithm that is used to assess a defendant's risk of re-offending -- racial bias [1] and a lack of auditing of other algorithms that affect our lives (cf. "Weapons of Math Destruction" by Cathy O'Neil) researchers started to investigate social aspects of predictive algorithms such as their Fairness, Accountability and Transparency (FAT).

A group of researchers at the University of Bristol, with sponsorship from Thales UK, has decided to investigate this problem further. Our goal is to democratise assessing social aspects and potentially detrimental effects of predictive models by providing an open source Python package (currently under development) licensed under BSD3 with easy to use Application Programming Interface (API) and minimal dependencies (SciPy and NumPy). The package is intended to be easily integrable with scikit-learn: the most popular and leading open source Python package for doing ML. With the abundance of novel FAT methods, a common open source framework could lower the entry barrier to this research field. Just like scikit-learn allows non-experts to fiddle around with state-of-the-art ML algorithms, we hope that the software framework that is the outcome of our research could allow a broader AI community and lay ML users to test and evaluate their algorithms for security issues, biases, discriminations and unexpected behaviour. Our package called FAT-Forensics will be a core component of your project.

If you decide to take up this project, over the next year you will be involved in developing back- and front-ends for the aforementioned package. You will work alongside AI and ML researchers from the University of Bristol to create a user-facing interface that can potentially impact how research in AI fairness, accountability and transparency is done. The service will be targeted at academic researchers, ML enthusiasts and developers who will use it to easily assess and investigate social aspect of their predictive models. The front-end, which you will build, will be responsible for loading new datasets to a database, retrieving existing datasets from the database, displaying and comparing single data points, generating interactive (and modular) reports of selected ML fairness, accountability and transparency metrics, providing interactive plots and exporting these reports into PDFs. The back-end will be responsible for storing and managing the database and calling the API of our Python package to populate the user-facing reports with desired metrics and data required to produce the interactive plots. (See below for a similar project done by Google.)

In addition to bespoke software engineering skills you will have an opportunity to learn about basic data visualisation as well as AI and ML concepts and algorithms. Overall, the project aims at creating an easy to use web application for a Python package that provides an API to assess fairness, accountability and transparency of ML algorithms with and additional goal of reproducing state-of-the-art results. This is a hot research topic at the moment with the potential of making a valuable contribution to the research community and with a possibility of having a high impact and becoming an open source standard for research in this area. Depending on your involvement and the direction that you want to take, you can also embark on creating custom visualisation plugins for Jupyter Lab -- interactive Python interface that works in a web browser. Alternatively you can propose to create or extend an existing tool that you believe would increase the impact of our project. Although it is not the main objective of the project, you can also get involved in shaping of the core FAT-Forensics package itself if you desire to get some hands-on ML experience. Nevertheless, please keep in mind that these are extra bits and pieces that you can get involved in once the Minimum Viable Product (MVP) stage of your project -- i.e. the web application -- has been reached. The only constraint for the project is that our package is written in Python, therefore whatever you develop has to be able to interact with it. The project will be open-sourced (based on the MIT licence) and you will maintain all the Intellectual Property (IP).

To ground this abstract project consider the aforementioned COMPAS discourse [1]. Since both ProPublica and Northpointe used different, and incompatible, fairness metrics neither of them were strictly right or wrong. With our tool and the web application that you will build as a part of this project, a lay user could load the dataset published by ProPublica, and click-by-click evaluate its fairness with the two metrics used by Northpointe and ProPublica to see that both of them yield different results without the need of writing or running any custom code on their own machine.

Since the inception of this project a similar effort has been made by Google (cf. [2] and [3]). They have released a plugin, called "what-if tool", for their TensorFlow (Google's ML package) visualisation framework called TensorBoard. Despite its similarity, it suffers from a severe limitation of interacting only with TensorFlow models. Given that it is a very specific ML tool and far from replacing the community-beloved scikit-learn, this project can have larger audience and higher impact than the one published by Google.

[1] https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/  
[2] https://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html  
[3] https://pair-code.github.io/what-if-tool/